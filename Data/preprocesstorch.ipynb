{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e6feefe-fa12-42a7-a12f-51df161da90a",
   "metadata": {},
   "source": [
    "# This torch Masking pipeline was created basrd on this [script](https://github.com/huggingface/transformers/blob/master/examples/flax/language-modeling/run_t5_mlm_flax.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "811aca3d-d0eb-437d-bd83-bc3a61bfb035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "args_dict = {\n",
    "  \"n_gpu\": 8,\n",
    "  \"model_type\": 't5',\n",
    "  \"config_name\":\"../configs\",\n",
    "  \"config_path\":\"../configs/t5-small.json\",\n",
    "  \"model_name_or_path\":\"../configs\",\n",
    "  \"tokenizer_name\":None,  \n",
    "  \"dataset_name\":\"wikitext\",\n",
    "  \"dataset_config_name\":\"wikitext-103-raw-v1\", \n",
    "  \"max_seq_length\": 128 ,\n",
    "  \"output_dir\": './test',\n",
    "  \"overwrite_output_dir\": True,\n",
    "  \"per_device_train_batch_size\": 10,\n",
    "  \"per_device_eval_batch_size\": 10,\n",
    "  \"learning_rate\": 0.005,\n",
    "  \"num_train_epochs\": 1,\n",
    "  \"do_train\": True,\n",
    "  \"do_eval\":True,\n",
    "  \"logging_steps\":500,\n",
    "  \"save_steps\":10000,\n",
    "  \"eval_steps\":500,\n",
    "  \"report_to\":\"wandb\",\n",
    "  \"evauation_strategy\":\"steps\",\n",
    "#   \"train_file_path\":\"Data/128train_data.pt\",\n",
    "#   \"valid_file_path\":\"Data/128valid_data.pt\",    \n",
    "}\n",
    "with open('args.json', 'w') as f:\n",
    "    json.dump(args_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1db99fac-ee53-4b6c-8352-75910cc59a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arij/anaconda3/envs/work/lib/python3.9/site-packages/jax/__init__.py:27: UserWarning: cloud_tpu_init failed: ConnectionError(MaxRetryError(\"HTTPConnectionPool(host='metadata.google.internal', port=80): Max retries exceeded with url: /computeMetadata/v1/instance/attributes/agent-worker-number (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f1b97d9a3d0>: Failed to establish a new connection: [Errno -2] Name or service not known'))\"))\n",
      " This a JAX bug; please report an issue at https://github.com/google/jax/issues\n",
      "  _warn(f\"cloud_tpu_init failed: {repr(exc)}\\n This a JAX bug; please report \"\n",
      "WARNING: Logging before InitGoogle() is written to STDERR\n",
      "I0220 10:30:34.624964 3248029 tpu_initializer_helper.cc:94] libtpu.so already in use by another process. Not attempting to load libtpu.so in this process.\n",
      "file ../tokenizer/config.json not found\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[10:30:35] - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 8distributed training: False, 16-bits training: False\n",
      "[10:30:35] - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=8,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=500,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.005,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./test/runs/Feb20_10-30-35_dgx3,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "output_dir=./test,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=10,\n",
      "per_device_train_batch_size=10,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['wandb'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./test,\n",
      "save_on_each_node=False,\n",
      "save_steps=10000,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "[10:30:35] - INFO - __main__ -   Model arguments ModelArguments(model_name_or_path='../configs', model_type='t5', config_name='../configs', config_path='../configs/t5-small.json', tokenizer_name=None, cache_dir=None, use_fast_tokenizer=True, dtype='float32')\n",
      "[10:30:35] - INFO - __main__ -   Data arguments DataTrainingArguments(dataset_name='wikitext', dataset_config_name='wikitext-103-raw-v1', train_file=None, validation_file=None, train_ref_file=None, validation_ref_file=None, overwrite_cache=False, validation_split_percentage=5, train_file_path=None, valid_file_path=None, max_seq_length=128, preprocessing_num_workers=None, mlm_probability=0.15, mean_noise_span_length=3.0)\n",
      "[INFO|tokenization_auto.py:341] 2022-02-20 10:30:35,273 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|tokenization_utils_base.py:1671] 2022-02-20 10:30:35,273 >> Didn't find file ../tokenizer/spiece.model. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1671] 2022-02-20 10:30:35,274 >> Didn't find file ../tokenizer/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1671] 2022-02-20 10:30:35,274 >> Didn't find file ../tokenizer/special_tokens_map.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1671] 2022-02-20 10:30:35,275 >> Didn't find file ../tokenizer/tokenizer_config.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1740] 2022-02-20 10:30:35,276 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1740] 2022-02-20 10:30:35,276 >> loading file ../tokenizer/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1740] 2022-02-20 10:30:35,276 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1740] 2022-02-20 10:30:35,277 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1740] 2022-02-20 10:30:35,277 >> loading file None\n",
      "[ERROR|configuration_utils.py:564] 2022-02-20 10:30:35,278 >> file ../tokenizer/config.json not found\n",
      "[WARNING|tokenization_utils_base.py:1933] 2022-02-20 10:30:35,312 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[10:30:35] - DEBUG - urllib3.connectionpool -   Starting new HTTPS connection (1): s3.amazonaws.com:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_args.train_file_path:  --------------------------------------------------> None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10:30:35] - DEBUG - urllib3.connectionpool -   https://s3.amazonaws.com:443 \"HEAD /datasets.huggingface.co/datasets/datasets/wikitext/wikitext.py HTTP/1.1\" 200 0\n",
      "[10:30:35] - DEBUG - urllib3.connectionpool -   Starting new HTTPS connection (1): raw.githubusercontent.com:443\n",
      "[10:30:36] - DEBUG - urllib3.connectionpool -   https://raw.githubusercontent.com:443 \"HEAD /huggingface/datasets/1.15.1/datasets/wikitext/wikitext.py HTTP/1.1\" 200 0\n",
      "[10:30:36] - DEBUG - urllib3.connectionpool -   Starting new HTTPS connection (1): raw.githubusercontent.com:443\n",
      "[10:30:36] - DEBUG - urllib3.connectionpool -   https://raw.githubusercontent.com:443 \"HEAD /huggingface/datasets/1.15.1/datasets/wikitext/dataset_infos.json HTTP/1.1\" 200 0\n",
      "[10:30:36] - INFO - datasets.info -   Loading Dataset Infos from /home/arij/.cache/huggingface/modules/datasets_modules/datasets/wikitext/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20\n",
      "[10:30:36] - INFO - datasets.builder -   Overwrite dataset info from restored data version.\n",
      "[10:30:36] - INFO - datasets.info -   Loading Dataset info from /home/arij/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20\n",
      "[10:30:36] - WARNING - datasets.builder -   Reusing dataset wikitext (/home/arij/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20)\n",
      "[10:30:36] - INFO - datasets.info -   Loading Dataset info from /home/arij/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e411fbfc1a4203ad29919b0ffb474a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10:30:36] - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/arij/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-5277237ce9f532e8.arrow\n",
      "[10:30:36] - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/arij/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-1e41464e3856dd1f.arrow\n",
      "[10:30:36] - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/arij/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-e32cf7eb2af5a901.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Tokenize data set  ************************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e161d36bbed4433cb445cb73acec13e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 128:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10:30:36] - INFO - datasets.arrow_dataset -   Caching processed dataset at /home/arij/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-94a903d644427d72.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f73faecc467a4687a677a266f0657bc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 128:   0%|          | 0/1802 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10:30:36] - INFO - datasets.arrow_dataset -   Caching processed dataset at /home/arij/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-32859008fe3fe8bd.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a947782f6564db9bddda7ccda3ad087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Grouping texts in chunks of 128:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10:31:39] - INFO - datasets.arrow_dataset -   Caching processed dataset at /home/arij/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/aa5e094000ec7afeb74c3be92c88313cd6f132d564c7effd961c10fd47c76f20/cache-b971abb1884cd9c7.arrow\n",
      "\n",
      "Training...:   0%|                                                                                                                                                                                                                                     | 0/1105878 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------   samples before masking using special tokens -------------------------------------------------------------------\n",
      "</s> = valkyria chronicles iii = </s></s> senjō no valkyria 3 : unrecorded chronicles ( japanese : 戦場のヴァルキュリア3, lit. valkyria of the battlefield 3 ), commonly referred to as valkyria chronicles iii outside japan, is a tactical role @-@ playing video game developed by sega and media.vision for the playstation portable. released in january 2011 in japan, it is the third game in the valkyria series. employing the same fusion of tactical\n",
      "**************************************************************************************************************************************\n",
      "---------------------------   input_ids  ------------------------------------\n",
      "</s> = valkyria chronicles iii = </s></s> senjō no valkyria 3 <extra_id_99> unrecorded chronicles ( japanese : 戦場のヴァ<extra_id_98>リア3, lit. valkyria of the battlefield 3 ), commonly referred to as valky<extra_id_97> iii outside japan, is a tactical role @-@ playing video<extra_id_96>d by sega and media.vision<extra_id_95>. released in january 2011 in japan, it is the third game in the valkyria<extra_id_94> employ<extra_id_93></s>\n",
      "---------------------------------------------------------------------------\n",
      "---------------------------------- labels ---------------------------------\n",
      "<extra_id_99>:<extra_id_98>ルキュ<extra_id_97>ria chronicles<extra_id_96> game develope<extra_id_95> for the playstation portable<extra_id_94> series.<extra_id_93>ing the same fusion of tactical</s>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training...:   0%|                                                                                                                                                                                                                        | 1/1105878 [00:04<1424:50:41,  4.64s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------   samples before masking using special tokens -------------------------------------------------------------------\n",
      "and real @-@ time gameplay as its predecessors, the story runs parallel to the first game and follows the \" nameless \", a penal military unit serving the nation of gallia during the second europan war who perform secret black operations and are pitted against the imperial unit \" calamaty raven \". </s> the game began development in 2010, carrying over a large portion of the work done on valkyria chronicles ii. while it retained the standard features of the series, it also underwent multiple adjustments, such as making the game more forgiving for series newcomers \n",
      "**************************************************************************************************************************************\n",
      "---------------------------   input_ids  ------------------------------------\n",
      "and real @-@ time gameplay as its predecessors, the story runs parallel to the first game and follows the \" nameless \", a penal military unit serving the nation of gallia<extra_id_99> the second europan war<extra_id_98> secret black operations and are pitted against the imperial unit \" calamaty raven \". <extra_id_97> in 2010, carrying over a large portion of the work done on<extra_id_96> while it retained<extra_id_95> standard features of the series, it also underwent<extra_id_94> adjustments, such as making the game more forgiving for series newcomers<extra_id_93></s>\n",
      "---------------------------------------------------------------------------\n",
      "---------------------------------- labels ---------------------------------\n",
      "<extra_id_99> during<extra_id_98> who perform<extra_id_97></s> the game began development<extra_id_96> valkyria chronicles ii.<extra_id_95> the<extra_id_94> multiple<extra_id_93> </s>\n",
      "CPU times: user 1min 3s, sys: 2.17 s, total: 1min 5s\n",
      "Wall time: 1min 12s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from torch.utils.data import DataLoader\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "import torch\n",
    "import transformers\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from itertools import chain\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "from huggingface_hub import Repository\n",
    "from transformers.models.t5.modeling_flax_t5 import shift_tokens_right\n",
    "\n",
    "from transformers.file_utils import get_full_repo_name\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_FOR_MASKED_LM_MAPPING,\n",
    "    AutoTokenizer,\n",
    "    BatchEncoding,\n",
    "    HfArgumentParser,\n",
    "    PreTrainedTokenizerBase,\n",
    "    T5Config,\n",
    "    TrainingArguments,\n",
    "    is_tensorboard_available,\n",
    "    set_seed,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    Trainer    \n",
    ")\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datasets import load_metric\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The model checkpoint for weights initialization.\"\n",
    "            \"Don't set if you want to train a model from scratch.\"\n",
    "        },\n",
    "    )\n",
    "    model_type: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" }, #+ \", \".join(MODEL_TYPES)},\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    config_path: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Full path if not the same as model_name\"}\n",
    "    )\n",
    "    \n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
    "    )\n",
    "    dtype: Optional[str] = field(\n",
    "        default=\"float32\",\n",
    "        metadata={\n",
    "            \"help\": \"Floating-point format in which the model weights should be initialized and trained. Choose one of `[float32, float16, bfloat16]`.\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n",
    "    validation_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n",
    "    )\n",
    "    train_ref_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"An optional input train ref data file for whole word masking in Chinese.\"},\n",
    "    )\n",
    "    validation_ref_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"An optional input validation ref data file for whole word masking in Chinese.\"},\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "    validation_split_percentage: Optional[int] = field(\n",
    "        default=5,\n",
    "        metadata={\n",
    "            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    train_file_path: Optional[str] = field(\n",
    "        default= None,\n",
    "        metadata={\"help\": \"Path for cached train dataset\"},\n",
    "    )\n",
    "    valid_file_path: Optional[str] = field(\n",
    "        default= None,\n",
    "        metadata={\"help\": \"Path for cached valid dataset\"},\n",
    "    )\n",
    "    \n",
    "    max_seq_length: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total input sequence length after tokenization and masking. Sequences longer than this will be truncated. Default to the max input length of the model.\"\n",
    "        },\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "    mlm_probability: float = field(\n",
    "        default=0.15, metadata={\"help\": \"Ratio of tokens to mask for span masked language modeling loss\"}\n",
    "    )\n",
    "    mean_noise_span_length: float = field(\n",
    "        default=3.0,\n",
    "        metadata={\"help\": \"Mean span length of masked tokens\"},\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "def process_datasets(model_args, data_args, training_args, tokenizer, expanded_inputs_length, save_path):\n",
    "    raw_datasets = None\n",
    "    if data_args.dataset_name is not None:\n",
    "        # Downloading and loading a dataset from the hub.\n",
    "        raw_datasets = load_dataset(\n",
    "            data_args.dataset_name, data_args.dataset_config_name, cache_dir=model_args.cache_dir\n",
    "        )\n",
    "        if \"validation\" not in raw_datasets.keys():\n",
    "            raw_datasets[\"validation\"] = load_dataset(\n",
    "                data_args.dataset_name,\n",
    "                data_args.dataset_config_name,\n",
    "                split=f\"train[:{data_args.validation_split_percentage}%]\",\n",
    "                cache_dir=model_args.cache_dir,\n",
    "            )\n",
    "            raw_datasets[\"train\"] = load_dataset(\n",
    "                data_args.dataset_name,\n",
    "                data_args.dataset_config_name,\n",
    "                split=f\"train[{data_args.validation_split_percentage}%:]\",\n",
    "                cache_dir=model_args.cache_dir,\n",
    "            )\n",
    "    else:\n",
    "        data_files = {}\n",
    "        if data_args.train_file is not None:\n",
    "            data_files[\"train\"] = data_args.train_file\n",
    "        if data_args.validation_file is not None:\n",
    "            data_files[\"validation\"] = data_args.validation_file\n",
    "        extension = data_args.train_file.split(\".\")[-1]\n",
    "        if extension == \"txt\":\n",
    "            extension = \"text\"\n",
    "        raw_datasets = load_dataset(extension, data_files=data_files, cache_dir=model_args.cache_dir)\n",
    "\n",
    "        # If no validation data is there, validation_split_percentage will be used to divide the dataset.\n",
    "        if \"validation\" not in raw_datasets.keys():\n",
    "            raw_datasets[\"validation\"] = load_dataset(\n",
    "                extension,\n",
    "                data_files=data_files,\n",
    "                split=f\"train[:{data_args.validation_split_percentage}%]\",\n",
    "                cache_dir=model_args.cache_dir,\n",
    "            )\n",
    "            raw_datasets[\"train\"] = load_dataset(\n",
    "                extension,\n",
    "                data_files=data_files,\n",
    "                split=f\"train[{data_args.validation_split_percentage}%:]\",\n",
    "                cache_dir=model_args.cache_dir,\n",
    "            )\n",
    "\n",
    "\n",
    "    # Preprocessing the datasets.\n",
    "    # First we tokenize all the texts.\n",
    "    if training_args.do_train:\n",
    "        column_names = raw_datasets[\"train\"].column_names\n",
    "    else:\n",
    "        column_names = raw_datasets[\"validation\"].column_names\n",
    "    text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "    if data_args.max_seq_length is None:\n",
    "        max_seq_length = tokenizer.model_max_length\n",
    "        if max_seq_length > 512:\n",
    "            logger.warning(\n",
    "                f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n",
    "                \"Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.\"\n",
    "            )\n",
    "            max_seq_length = 512\n",
    "    else:\n",
    "        if data_args.max_seq_length > tokenizer.model_max_length:\n",
    "            logger.warning(\n",
    "                f\"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the\"\n",
    "                f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n",
    "            )\n",
    "        max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n",
    "        \n",
    "\n",
    "    def tokenize_function(examples):\n",
    "\n",
    "        return tokenizer(examples[text_column_name], return_attention_mask=False)\n",
    "\n",
    "    print(\"******************** Tokenize data set  ************************\")\n",
    "    with training_args.main_process_first(desc=\"dataset map tokenization\"):\n",
    "        tokenized_datasets = raw_datasets.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            remove_columns=column_names,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on every text in dataset\",\n",
    "        )\n",
    "    # Main data processing function that will concatenate all texts from our dataset and generate chunks of\n",
    "    # expanded_inputs_length.\n",
    "    def group_texts(examples):\n",
    "        # Concatenate all texts.\n",
    "        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "        # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "        if total_length >= expanded_inputs_length:\n",
    "            total_length = (total_length // expanded_inputs_length) * expanded_inputs_length\n",
    "        # Split by chunks of max_len.\n",
    "        result = {\n",
    "            k: [t[i : i + expanded_inputs_length] for i in range(0, total_length, expanded_inputs_length)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "        return result\n",
    "\n",
    "        # Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a\n",
    "        # remainder for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value\n",
    "        # might be slower to preprocess.\n",
    "        #\n",
    "        # To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n",
    "        # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n",
    "\n",
    "    with training_args.main_process_first(desc=\"grouping texts together\"):\n",
    "        tokenized_datasets = tokenized_datasets.map(\n",
    "            group_texts,\n",
    "            batched=True,\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=f\"Grouping texts in chunks of {max_seq_length}\",\n",
    "        )\n",
    "\n",
    "    if training_args.do_train:\n",
    "        if \"train\" not in tokenized_datasets:\n",
    "            raise ValueError(\"--do_train requires a train dataset\")\n",
    "        train_dataset = tokenized_datasets[\"train\"]\n",
    "#         if data_args.max_train_samples is not None:\n",
    "#             train_dataset = train_dataset.select(range(data_args.max_train_samples))\n",
    "\n",
    "    if training_args.do_eval:\n",
    "        if \"validation\" not in tokenized_datasets:\n",
    "            raise ValueError(\"--do_eval requires a validation dataset\")\n",
    "        eval_dataset = tokenized_datasets[\"validation\"]\n",
    "\n",
    "    if \"test\" not in tokenized_datasets:\n",
    "        raise ValueError(\"--do_predict requires a test dataset\")\n",
    "    test_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "    # cach the dataset, so we can load it directly for training\n",
    "    torch.save(train_dataset, save_path+str(max_seq_length)+'train_data.pt') \n",
    "    torch.save(eval_dataset, save_path+str(max_seq_length)+'valid_data.pt')\n",
    "    torch.save(test_dataset, save_path+str(max_seq_length)+'test_data.pt')\n",
    "    return train_dataset, eval_dataset, test_dataset\n",
    "    \n",
    "    \n",
    "def create_tokenizer(model_args):\n",
    "    config = None \n",
    "    config = T5Config.from_json_file(json_file=model_args.config_path)\n",
    "   \n",
    "    if model_args.tokenizer_name:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_args.tokenizer_name, cache_dir=model_args.cache_dir, use_fast=model_args.use_fast_tokenizer\n",
    "        )\n",
    "        return tokenizer\n",
    "    elif model_args.model_name_or_path:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_path, config=config)\n",
    "\n",
    "        return tokenizer\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
    "            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
    "        )\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForT5MLM:\n",
    "    \"\"\"\n",
    "    Data collator used for T5 span-masked language modeling.\n",
    "    It is made sure that after masking the inputs are of length `data_args.max_seq_length` and targets are also of fixed length.\n",
    "    For more information on how T5 span-masked language modeling works, one can take a look\n",
    "    at the `official paper <https://arxiv.org/pdf/1910.10683.pdf>`__\n",
    "    or the `official code for preprocessing <https://github.com/google-research/text-to-text-transfer-transformer/blob/master/t5/data/preprocessors.py>`__ .\n",
    "    Args:\n",
    "        tokenizer (:class:`~transformers.PreTrainedTokenizer` or :class:`~transformers.PreTrainedTokenizerFast`):\n",
    "            The tokenizer used for encoding the data.\n",
    "        noise_density (:obj:`float`):\n",
    "            The probability with which to (randomly) mask tokens in the input.\n",
    "        mean_noise_span_length (:obj:`float`):\n",
    "            The average span length of the masked tokens.\n",
    "        input_length (:obj:`int`):\n",
    "            The expected input length after masking.\n",
    "        target_length (:obj:`int`):\n",
    "            The expected target length after masking.\n",
    "        pad_token_id: (:obj:`int`):\n",
    "            The pad token id of the model\n",
    "        decoder_start_token_id: (:obj:`int):\n",
    "            The decoder start token id of the model\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    noise_density: float\n",
    "    mean_noise_span_length: float\n",
    "    input_length: int\n",
    "    target_length: int\n",
    "    pad_token_id: int\n",
    "\n",
    "    def __call__(self, examples: List[Dict[str, np.ndarray]]) -> Dict[str, np.ndarray]:\n",
    "        '''\n",
    "        1. random_spans_noise_mask\n",
    "        2. random span noise \n",
    "        3. create_sentinel_ids\n",
    "        4. filter_input_ids\n",
    "        if max length = 512 then len(example['input_ids']) = 568\n",
    "        '''\n",
    "        \n",
    "        batch = BatchEncoding( # batch.keys() = input_ids\n",
    "            {k: np.array([examples[i][k] for i in range(len(examples))]) for k, v in examples[0].items()}\n",
    "        )\n",
    "        print('-----------------------------------------------------------------   samples before masking using special tokens -------------------------------------------------------------------')\n",
    "        for i , x in enumerate(batch[\"input_ids\"]):\n",
    "            print(self.tokenizer.decode(x))\n",
    "        print(\"**************************************************************************************************************************************\")   \n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        batch_size, expandend_input_length = input_ids.shape\n",
    "        mask_indices = np.asarray([self.random_spans_noise_mask(expandend_input_length) for i in range(batch_size)])\n",
    "        labels_mask = ~mask_indices\n",
    "        input_ids_sentinel = self.create_sentinel_ids(mask_indices.astype(np.int8))\n",
    "        labels_sentinel = self.create_sentinel_ids(labels_mask.astype(np.int8))\n",
    "        inputs = self.filter_input_ids(input_ids, input_ids_sentinel)\n",
    "        batch[\"input_ids\"] = torch.tensor(inputs)\n",
    "        labels = self.filter_input_ids(input_ids, labels_sentinel)\n",
    "        batch[\"labels\"] = torch.tensor(labels)\n",
    "        if batch[\"input_ids\"].shape[-1] != self.input_length:\n",
    "            raise ValueError(\n",
    "                f\"`input_ids` are incorrectly preprocessed. `input_ids` length is {batch['input_ids'].shape[-1]}, but should be {self.target_length}.\"\n",
    "            )\n",
    "        if batch[\"labels\"].shape[-1] != self.target_length:\n",
    "            raise ValueError(\n",
    "                f\"`labels` are incorrectly preprocessed. `labels` length is {batch['labels'].shape[-1]}, but should be {self.target_length}.\"\n",
    "            )\n",
    "        # to check that tokens are correctly proprocessed, one can run `self.tokenizer.batch_decode(input_ids)` and `self.tokenizer.batch_decode(labels)` here...\n",
    "        print('---------------------------   input_ids  ------------------------------------')\n",
    "        for i , x in enumerate(batch[\"input_ids\"]):\n",
    "            print(self.tokenizer.decode(x))\n",
    "        print(\"---------------------------------------------------------------------------\")\n",
    "        print('---------------------------------- labels ---------------------------------')\n",
    "        for i,x in enumerate(batch[\"labels\"]):\n",
    "            print(self.tokenizer.decode(x))\n",
    "#         print(\"self.tokenizer.batch_decode(labels):  \",  self.tokenizer.decode(labels))   \n",
    "        # to check that tokens are correctly proprocessed, one can run `self.tokenizer.batch_decode(input_ids)` and `self.tokenizer.batch_decode(labels)` here...\n",
    "#         batch[\"decoder_input_ids\"] = torch.tensor(shift_tokens_right(\n",
    "#             labels, self.pad_token_id, self.decoder_start_token_id\n",
    "#         ))\n",
    "\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def create_sentinel_ids(self, mask_indices):\n",
    "        \"\"\"\n",
    "        Sentinel ids creation given the indices that should be masked.\n",
    "        The start indices of each mask are replaced by the sentinel ids in increasing\n",
    "        order. Consecutive mask indices to be deleted are replaced with `-1`.\n",
    "        \"\"\"\n",
    "        start_indices = mask_indices - np.roll(mask_indices, 1, axis=-1) * mask_indices\n",
    "        start_indices[:, 0] = mask_indices[:, 0]\n",
    "        sentinel_ids = np.where(start_indices != 0, np.cumsum(start_indices, axis=-1), start_indices)\n",
    "        sentinel_ids = np.where(sentinel_ids != 0, (len(self.tokenizer) - sentinel_ids), 0)\n",
    "        sentinel_ids -= mask_indices - start_indices\n",
    "\n",
    "        return sentinel_ids\n",
    "\n",
    "    def filter_input_ids(self, input_ids, sentinel_ids):\n",
    "        \"\"\"\n",
    "        Puts sentinel mask on `input_ids` and fuse consecutive mask tokens into a single mask token by deleting.\n",
    "        This will reduce the sequence length from `expanded_inputs_length` to `input_length`.\n",
    "        \"\"\"\n",
    "        batch_size = input_ids.shape[0]\n",
    "        input_ids_full = np.where(sentinel_ids != 0, sentinel_ids, input_ids)\n",
    "        input_ids = input_ids_full[input_ids_full > 0].reshape((batch_size, -1))\n",
    "        input_ids = np.concatenate(\n",
    "            [input_ids, np.full((batch_size, 1), self.tokenizer.eos_token_id, dtype=np.int32)], axis=-1\n",
    "        )\n",
    "        return input_ids\n",
    "\n",
    "    def random_spans_noise_mask(self, length):\n",
    "\n",
    "        \"\"\"This function is copy of `random_spans_helper <https://github.com/google-research/text-to-text-transfer-transformer/blob/84f8bcc14b5f2c03de51bd3587609ba8f6bbd1cd/t5/data/preprocessors.py#L2682>`__ .\n",
    "        Noise mask consisting of random spans of noise tokens.\n",
    "        The number of noise tokens and the number of noise spans and non-noise spans\n",
    "        are determined deterministically as follows:\n",
    "        num_noise_tokens = round(length * noise_density)\n",
    "        num_nonnoise_spans = num_noise_spans = round(num_noise_tokens / mean_noise_span_length)\n",
    "        Spans alternate between non-noise and noise, beginning with non-noise.\n",
    "        Subject to the above restrictions, all masks are equally likely.\n",
    "        Args:\n",
    "            length: an int32 scalar (length of the incoming token sequence)\n",
    "            noise_density: a float - approximate density of output mask\n",
    "            mean_noise_span_length: a number\n",
    "        Returns:\n",
    "            a boolean tensor with shape [length]\n",
    "        \"\"\"\n",
    "        orig_length = length\n",
    "        num_noise_tokens = int(np.round(length * self.noise_density))\n",
    "        # avoid degeneracy by ensuring positive numbers of noise and nonnoise tokens.\n",
    "        num_noise_tokens = min(max(num_noise_tokens, 1), length - 1)\n",
    "        num_noise_spans = int(np.round(num_noise_tokens / self.mean_noise_span_length))\n",
    "\n",
    "        # avoid degeneracy by ensuring positive number of noise spans\n",
    "        num_noise_spans = max(num_noise_spans, 1)\n",
    "        num_nonnoise_tokens = length - num_noise_tokens\n",
    "\n",
    "        # pick the lengths of the noise spans and the non-noise spans\n",
    "        def _random_segmentation(num_items, num_segments):\n",
    "            \"\"\"Partition a sequence of items randomly into non-empty segments.\n",
    "            Args:\n",
    "                num_items: an integer scalar > 0\n",
    "                num_segments: an integer scalar in [1, num_items]\n",
    "            Returns:\n",
    "                a Tensor with shape [num_segments] containing positive integers that add\n",
    "                up to num_items\n",
    "            \"\"\"\n",
    "            mask_indices = np.arange(num_items - 1) < (num_segments - 1)\n",
    "            np.random.shuffle(mask_indices)\n",
    "            first_in_segment = np.pad(mask_indices, [[1, 0]])\n",
    "            segment_id = np.cumsum(first_in_segment)\n",
    "            # count length of sub segments assuming that list is sorted\n",
    "            _, segment_length = np.unique(segment_id, return_counts=True)\n",
    "            return segment_length\n",
    "\n",
    "        noise_span_lengths = _random_segmentation(num_noise_tokens, num_noise_spans)\n",
    "        nonnoise_span_lengths = _random_segmentation(num_nonnoise_tokens, num_noise_spans)\n",
    "\n",
    "        interleaved_span_lengths = np.reshape(\n",
    "            np.stack([nonnoise_span_lengths, noise_span_lengths], axis=1), [num_noise_spans * 2]\n",
    "        )\n",
    "        span_starts = np.cumsum(interleaved_span_lengths)[:-1]\n",
    "        span_start_indicator = np.zeros((length,), dtype=np.int8)\n",
    "        span_start_indicator[span_starts] = True\n",
    "        span_num = np.cumsum(span_start_indicator)\n",
    "        is_noise = np.equal(span_num % 2, 1)\n",
    "        return is_noise[:orig_length]\n",
    "          \n",
    "def compute_input_and_target_lengths(inputs_length, noise_density, mean_noise_span_length):\n",
    "    \"\"\"This function is copy of `random_spans_helper <https://github.com/google-research/text-to-text-transfer-transformer/blob/84f8bcc14b5f2c03de51bd3587609ba8f6bbd1cd/t5/data/preprocessors.py#L2466>`__ .\n",
    "    Training parameters to avoid padding with random_spans_noise_mask.\n",
    "    When training a model with random_spans_noise_mask, we would like to set the other\n",
    "    training hyperparmeters in a way that avoids padding.\n",
    "    This function helps us compute these hyperparameters.\n",
    "    We assume that each noise span in the input is replaced by extra_tokens_per_span_inputs sentinel tokens,\n",
    "    and each non-noise span in the targets is replaced by extra_tokens_per_span_targets sentinel tokens.\n",
    "    This function tells us the required number of tokens in the raw example (for split_tokens())\n",
    "    as well as the length of the encoded targets. Note that this function assumes\n",
    "    the inputs and targets will have EOS appended and includes that in the reported length.\n",
    "    Args:\n",
    "        inputs_length: an integer - desired length of the tokenized inputs sequence\n",
    "        noise_density: a float\n",
    "        mean_noise_span_length: a float\n",
    "    Returns:\n",
    "        tokens_length: length of original text in tokens\n",
    "        targets_length: an integer - length in tokens of encoded targets sequence\n",
    "    \"\"\"\n",
    "\n",
    "    def _tokens_length_to_inputs_length_targets_length(tokens_length):\n",
    "        num_noise_tokens = int(round(tokens_length * noise_density))\n",
    "        num_nonnoise_tokens = tokens_length - num_noise_tokens\n",
    "        num_noise_spans = int(round(num_noise_tokens / mean_noise_span_length))\n",
    "        # inputs contain all nonnoise tokens, sentinels for all noise spans\n",
    "        # and one EOS token.\n",
    "        _input_length = num_nonnoise_tokens + num_noise_spans + 1\n",
    "        _output_length = num_noise_tokens + num_noise_spans + 1\n",
    "        return _input_length, _output_length\n",
    "\n",
    "    tokens_length = inputs_length\n",
    "\n",
    "    while _tokens_length_to_inputs_length_targets_length(tokens_length + 1)[0] <= inputs_length:\n",
    "        tokens_length += 1\n",
    "\n",
    "    inputs_length, targets_length = _tokens_length_to_inputs_length_targets_length(tokens_length)\n",
    "\n",
    "    # minor hack to get the targets length to be equal to inputs length\n",
    "    # which is more likely to have been set to a nice round number.\n",
    "    if noise_density == 0.5 and targets_length > inputs_length:\n",
    "        tokens_length -= 1\n",
    "        targets_length -= 1\n",
    "    return tokens_length, targets_length\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 1: Parse argumen\n",
    "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "    model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath('args.json'))\n",
    "        \n",
    "    tokenizer = create_tokenizer(model_args)\n",
    "\n",
    "\n",
    "    # 2: Otput directory is exist and empty\n",
    "    if (\n",
    "        os.path.exists(training_args.output_dir)\n",
    "        and os.listdir(training_args.output_dir)\n",
    "        and training_args.do_train\n",
    "        and not training_args.overwrite_output_dir\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty.\"\n",
    "            \"Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "\n",
    "    #3:  Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        level=\"NOTSET\",\n",
    "        datefmt=\"[%X]\",\n",
    "    )\n",
    "    # Log on each process the small summary:\n",
    "    logger = logging.getLogger(__name__)\n",
    "    log_level = training_args.get_process_log_level()\n",
    "    logger.setLevel(log_level)\n",
    "    datasets.utils.logging.set_verbosity(log_level)\n",
    "    transformers.utils.logging.set_verbosity(log_level)\n",
    "    transformers.utils.logging.enable_default_handler()\n",
    "    transformers.utils.logging.enable_explicit_format()\n",
    "    # Log on each process the small summary:\n",
    "    logger.warning(\n",
    "        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    "    )\n",
    "    # Set the verbosity to info of the Transformers logger (on main process only):\n",
    "    logger.info(f\"Training/evaluation parameters {training_args}\")\n",
    "    logger.info(f\"Model arguments {model_args}\")\n",
    "    logger.info(f\"Data arguments {data_args}\")\n",
    "    \n",
    "    #4:  Set seed before initializing model.\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "    #5:  Detecting last checkpoint.\n",
    "    last_checkpoint = None\n",
    "    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "            raise ValueError(\n",
    "                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "                \"Use --overwrite_output_dir to overcome.\"\n",
    "            )\n",
    "        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "            logger.info(\n",
    "                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "            )\n",
    "\n",
    "    #6:  create  tokenizer.\n",
    "    tokenizer = create_tokenizer(model_args)    \n",
    "    if data_args.max_seq_length is None:\n",
    "        max_seq_length = tokenizer.model_max_length\n",
    "        if max_seq_length > 512:\n",
    "            logger.warning(\n",
    "                f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n",
    "                \"Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.\"\n",
    "            )\n",
    "            max_seq_length = 512\n",
    "    else:\n",
    "        if data_args.max_seq_length > tokenizer.model_max_length:\n",
    "            logger.warning(\n",
    "                f\"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the\"\n",
    "                f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n",
    "            )\n",
    "        max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n",
    "\n",
    "    # 7: T5-like span masked language modeling will fuse consecutively masked tokens to a single sentinel token.\n",
    "    # To ensure that the input length is `max_seq_length`, we need to increase the maximum length\n",
    "    # according to `mlm_probability` and `mean_noise_span_length`. We can also define the label length accordingly.\n",
    "    expanded_inputs_length, targets_length = compute_input_and_target_lengths(\n",
    "        inputs_length=max_seq_length,\n",
    "        noise_density=data_args.mlm_probability,\n",
    "        mean_noise_span_length=data_args.mean_noise_span_length,\n",
    "    )\n",
    "\n",
    "    #8:  Get datasets if they already exists or download and prepare them for using\n",
    "    train_dataset, eval_dataset = None, None\n",
    "    print('data_args.train_file_path:  -------------------------------------------------->', data_args.train_file_path)\n",
    "    if data_args.train_file_path != None and data_args.valid_file_path != None:\n",
    "        print('loading data')\n",
    "        train_dataset  = torch.load(data_args.train_file_path)\n",
    "        eval_dataset = torch.load(data_args.valid_file_path)\n",
    "        print('loading done')    \n",
    "    elif data_args.dataset_name is not None or data_args.train_file is not None: \n",
    "        train_dataset, eval_dataset, test_dataset = process_datasets(model_args, \n",
    "                                                       data_args, \n",
    "                                                       training_args, \n",
    "                                                       tokenizer, \n",
    "                                                       expanded_inputs_length, \n",
    "                                                       \"../Data/wiki_103/\")        \n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"No available datasets. You need to load a cashed dataset or process a dataset.\"\n",
    "        )  \n",
    "    \n",
    "    # 9: create model     \n",
    "#     model = create_model(model_args, tokenizer) \n",
    "\n",
    "    # 10: Data collator\n",
    "    # This one will take care of randomly masking the tokens.\n",
    "    data_collator = DataCollatorForT5MLM(\n",
    "        tokenizer=tokenizer,\n",
    "        noise_density=data_args.mlm_probability,\n",
    "        mean_noise_span_length=data_args.mean_noise_span_length,\n",
    "        input_length=max_seq_length,\n",
    "        target_length=targets_length,\n",
    "        pad_token_id=0  # model.config.pad_token_id,\n",
    "    )\n",
    "    train_dataloader = DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=1,\n",
    "                collate_fn=data_collator,\n",
    "                num_workers=0,\n",
    "                pin_memory=True,\n",
    "            )\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Training...\", position=1)):\n",
    "        if step == 1:\n",
    "            break   \n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b427ec02-d471-40bf-8a82-7877d04d35d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' = valkyria chronicles iii =  senjō no valkyria 3 : unrecorded chronicles ( japanese : 戦場のヴァルキュリア3, lit. valkyria of the battlefield 3 ), commonly referred to as valkyria chronicles iii outside japan, is a tactical role @-@ playing video game developed by sega and media.vision for the playstation portable. released in january 2011 in japan, it is the third game in the valkyria series. employing the same fusion of tactical and real @-@ time gameplay as its predecessors, the story runs parallel to the first game and follows the \" nameless \", a penal military unit serving the nation of gallia during the second europan war who perform secret black operations and are pitted against the imperial unit \" calamaty raven \".  the game began development in 2010, carrying over a large portion of the work done on valkyria chronicles ii. while it retained the standard features of the series, it also underwent multiple adjustments, such as making the game more forgiving for series newcomers. character designer raita honjou and composer hitoshi sakimoto both returned from previous entries, along with valkyria chronicles ii director takeshi ozawa. a large team of writers handled the script. the game\\'s opening theme was sung by may \\'n.  it met with positive sales in japan, and was praised by both japanese and western critics. after release, it received downloadable content, along with an expanded edition in november of that year. it was also adapted into manga and an original video animation series. due to low sales of valkyria chronicles ii, valkyria chronicles iii was not localized, but a fan translation compatible with the game\\'s expanded edition was released in 2014. media.vision would return to the franchise with the development of valkyria : azure revolution for the playstation 4.  = = gameplay = =  as with previous valkyira chronicles games, valkyria chronicles iii is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces. stories are told through comic book '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "line = '''</s> = valkyria chronicles iii = </s></s> senjō no valkyria 3 : unrecorded chronicles ( japanese : 戦場のヴァルキュリア3, lit. valkyria of the battlefield 3 ), commonly referred to as valkyria chronicles iii outside japan, is a tactical role @-@ playing video game developed by sega and media.vision for the playstation portable. released in january 2011 in japan, it is the third game in the valkyria series. employing the same fusion of tactical and real @-@ time gameplay as its predecessors, the story runs parallel to the first game and follows the \" nameless \", a penal military unit serving the nation of gallia during the second europan war who perform secret black operations and are pitted against the imperial unit \" calamaty raven \". </s> the game began development in 2010, carrying over a large portion of the work done on valkyria chronicles ii. while it retained the standard features of the series, it also underwent multiple adjustments, such as making the game more forgiving for series newcomers. character designer raita honjou and composer hitoshi sakimoto both returned from previous entries, along with valkyria chronicles ii director takeshi ozawa. a large team of writers handled the script. the game's opening theme was sung by may 'n. </s> it met with positive sales in japan, and was praised by both japanese and western critics. after release, it received downloadable content, along with an expanded edition in november of that year. it was also adapted into manga and an original video animation series. due to low sales of valkyria chronicles ii, valkyria chronicles iii was not localized, but a fan translation compatible with the game's expanded edition was released in 2014. media.vision would return to the franchise with the development of valkyria : azure revolution for the playstation 4. </s></s> = = gameplay = = </s></s> as with previous valkyira chronicles games, valkyria chronicles iii is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces. stories are told through comic book '''\n",
    "line.replace(\"</s>\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55f6307f-4d3f-4917-956a-511879f68e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " = valkyria chronicles iii = \n",
      "\n",
      " senjō no valkyria 3 : unrecorded chronicles ( japanese : 戦場のヴァルキュリア3, lit. valkyria of the battlefield 3 ), commonly referred to as valkyria chronicles iii outside japan, is a tactical role @-@ playing video game developed by sega and media.vision for the playstation portable. released in january 2011 in japan, it is the third game in the valkyria series. employing the same fusion of tactical and real @-@ time gameplay as its predecessors, the story runs parallel to the first game and follows the \" nameless \", a penal military unit serving the nation of gallia during the second europan war who perform secret black operations and are pitted against the imperial unit \" calamaty raven \". \n",
      " the game began development in 2010, carrying over a large portion of the work done on valkyria chronicles ii. while it retained the standard features of the series, it also underwent multiple adjustments, such as making the game more forgiving for series newcomers. character designer raita honjou and composer hitoshi sakimoto both returned from previous entries, along with valkyria chronicles ii director takeshi ozawa. a large team of writers handled the script. the game's opening theme was sung by may 'n. \n",
      " it met with positive sales in japan, and was praised by both japanese and western critics. after release, it received downloadable content, along with an expanded edition in november of that year. it was also adapted into manga and an original video animation series. due to low sales of valkyria chronicles ii, valkyria chronicles iii was not localized, but a fan translation compatible with the game's expanded edition was released in 2014. media.vision would return to the franchise with the development of valkyria : azure revolution for the playstation 4. \n",
      "\n",
      " = = gameplay = = \n",
      "\n",
      " as with previous valkyira chronicles games, valkyria chronicles iii is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces. stories are told through comic book \n"
     ]
    }
   ],
   "source": [
    "print(line.replace(\"</s>\",\"\\n\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
